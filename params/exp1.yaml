batch size: 128
device: cuda:0
dataroot: ../input/dataset/
dropout: 0.5
epoch: 60
optim: sgd
lr: 0.1
scheduler: torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 30, 40], gamma=0.1)
thread: 14
seed: 2019
fold: [0,1,2,3,4]
save path: /mnt/hdd1/alcon2019/